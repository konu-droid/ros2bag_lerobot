{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to load dataset for inference\n",
    "\n",
    "\n",
    "## LeRobot Format\n",
    "\n",
    "* This tutorial will show how to load data in LeRobot Format by using our dataloader. \n",
    "* We will use the `robot_sim.PickNPlace` dataset as an example which is already converted to LeRobot Format. \n",
    "* To understand how to convert your own dataset, please refer to [Gr00t's LeRobot.md](LeRobot_compatible_data_schema.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.misc import any_describe\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.data.dataset import ModalityConfig\n",
    "from gr00t.data.schema import EmbodimentTag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We need to define 3 things to load the dataset:\n",
    "1. Path to the dataset\n",
    "\n",
    "2. `ModalityConfigs`\n",
    "\n",
    "- `ModalityConfigs` defines what data modalities (like video, state, actions, language) to use downstream like model training or inference.\n",
    "- Each modality specifies which frame to load via delta_indices (e.g. [0] means current frame only, [-1,0] means previous and current frame)\n",
    "\n",
    "3. `EmbodimentTag`\n",
    "- `EmbodimentTag` is used to specify the embodiment of the dataset. A list of all the embodiment tags can be found in `gr00t.data.embodiment_tags.EmbodimentTag`.\n",
    "- GR00T's architecture has different action heads optimized for specific robot types (embodiments). `EmbodimentTag` tells the model which action head to use for fine-tuning and/or inference. In our case, since we're using a humanoid arm, we specify `EmbodimentTag.GR1_UNIFIED` to get the best performance from the humanoid-specific action head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import gr00t\n",
    "\n",
    "# REPO_PATH is the path of the pip install gr00t repo and one level up\n",
    "REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
    "DATA_PATH = os.path.join(REPO_PATH, \"demo_data/robot_sim.PickNPlace\")\n",
    "# DATA_PATH = \"/home/konu/Documents/groot/Isaac-GR00T-ros2/my_robot_groot_lerobot_dataset\"\n",
    "\n",
    "print(\"Loading dataset... from\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. modality configs\n",
    "modality_configs = {\n",
    "    \"video\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\"video.ego_view\"],\n",
    "    ),\n",
    "    \"state\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"state.left_arm\",\n",
    "            \"state.left_hand\",\n",
    "            \"state.left_leg\",\n",
    "            \"state.neck\",\n",
    "            \"state.right_arm\",\n",
    "            \"state.right_hand\",\n",
    "            \"state.right_leg\",\n",
    "            \"state.waist\",\n",
    "        ],\n",
    "    ),\n",
    "    \"action\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"action.left_hand\",\n",
    "            \"action.right_hand\",\n",
    "        ],\n",
    "    ),\n",
    "    \"language\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\"annotation.human.action.task_description\", \"annotation.human.validity\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. gr00t embodiment tag\n",
    "embodiment_tag = EmbodimentTag.GR1\n",
    "\n",
    "# load the dataset\n",
    "dataset = LeRobotSingleDataset(DATA_PATH, modality_configs,  embodiment_tag=embodiment_tag)\n",
    "\n",
    "print('\\n'*2)\n",
    "print(\"=\"*100)\n",
    "print(f\"{' Humanoid Dataset ':=^100}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# print the 7th data point\n",
    "resp = dataset[7]\n",
    "any_describe(resp)\n",
    "print(resp.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Image frames within the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        resp = dataset[i]\n",
    "        img = resp[\"video.ego_view\"][0]\n",
    "        images_list.append(img)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(images_list[i])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Image {i}\")\n",
    "plt.tight_layout() # adjust the subplots to fit into the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "We can also apply a series of transformation to the data to our `LeRobotSingleDataset` class. This shows how to apply transformations to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.transform.base import ComposedModalityTransform\n",
    "from gr00t.data.transform import VideoToTensor, VideoCrop, VideoResize, VideoColorJitter, VideoToNumpy\n",
    "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
    "from gr00t.data.transform.concat import ConcatTransform\n",
    "\n",
    "\n",
    "video_modality = modality_configs[\"video\"]\n",
    "state_modality = modality_configs[\"state\"]\n",
    "action_modality = modality_configs[\"action\"]\n",
    "\n",
    "# select the transforms you want to apply to the data\n",
    "to_apply_transforms = ComposedModalityTransform(\n",
    "    transforms=[\n",
    "        # video transforms\n",
    "        VideoToTensor(apply_to=video_modality.modality_keys),\n",
    "        VideoCrop(apply_to=video_modality.modality_keys, scale=0.95),\n",
    "        VideoResize(apply_to=video_modality.modality_keys, height=224, width=224, interpolation=\"linear\"),\n",
    "        VideoColorJitter(apply_to=video_modality.modality_keys, brightness=0.3, contrast=0.4, saturation=0.5, hue=0.08),\n",
    "        VideoToNumpy(apply_to=video_modality.modality_keys),\n",
    "\n",
    "        # state transforms\n",
    "        StateActionToTensor(apply_to=state_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=state_modality.modality_keys, normalization_modes={\n",
    "            key: \"min_max\" for key in state_modality.modality_keys\n",
    "        }),\n",
    "\n",
    "        # action transforms\n",
    "        StateActionToTensor(apply_to=action_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=action_modality.modality_keys, normalization_modes={\n",
    "            key: \"min_max\" for key in action_modality.modality_keys\n",
    "        }),\n",
    "\n",
    "        # ConcatTransform\n",
    "        ConcatTransform(\n",
    "            video_concat_order=video_modality.modality_keys,\n",
    "            state_concat_order=state_modality.modality_keys,\n",
    "            action_concat_order=action_modality.modality_keys,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how the data is different after applying the transformations.\n",
    "\n",
    "e.g. states and actions are being normalized and concatenated, video images are being cropped, resized, and color-jittered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LeRobotSingleDataset(\n",
    "    DATA_PATH,\n",
    "    modality_configs,\n",
    "    transforms=to_apply_transforms,\n",
    "    embodiment_tag=embodiment_tag\n",
    ")\n",
    "\n",
    "# print the 7th data point\n",
    "resp = dataset[7]\n",
    "any_describe(resp)\n",
    "print(resp.keys())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
